{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <table><tr><td with='5'></td># Implementing logistic Regression from Scratch</td></tr>\n",
    " <tr><td background-color:'gray'></td><td>Created on Fri Apr 15 12:02:15 2016</td></tr>\n",
    " <tr><td></td><td>\n",
    " \n",
    " The goal of this assignment is to implement your own logistic regression classifier.\n",
    "<ol>\n",
    "    <li> extract features from Amazon product Reviews\n",
    "    <li> Convert dataFrame into a Numpy array\n",
    "    <li> Implement the link function for logistic regression\n",
    "    <li> Write a function to compute the derivative of the log likelihood function with respect to a single coefficient.\n",
    "    <li> implement gradient ascent\n",
    "    <li> Given a set of coefficients, predict sentiments\n",
    "    <li> Compute classification accuracy for the logistic regression model\n",
    "</ol>\n",
    "  \n",
    " </td></tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@author: jmlbeaujour@gmail.com\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions:\n",
    "<ul>\n",
    " <li> remove_punctuation(text)\n",
    " Takes a line of text and removes all punctuation\n",
    " <li> dataFrame2Array(dataframe, features, label)\n",
    " return 2 arrays: 2D array feature_matrix, label_array i.e real sentiment\n",
    " <li> predict_probability(feature_matrix, coefficients)\n",
    " <li> feature_derivative(errors, feature)\n",
    " <li> compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    " <li> logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(None, string.punctuation)   \n",
    "\n",
    "def dataFrame2Array(dataframe, features, label):\n",
    "    dataframe['constant'] = 1 #add the bias term\n",
    "    #add 'contains_' to all features to match column name #features is a list of string\n",
    "    features = ['constant']+['contains_'+x for x in features] \n",
    "    feature_matrix = np.array(dataframe[features]) #extract set of columns from dataframe and convert to matrix\n",
    "    label_array = np.array(dataframe[label]) #create an array with the label (actual value)\n",
    "    label_array = np.reshape(label_array, (np.shape(label_array)[0],1))\n",
    "    return (feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate conditional probability with link function:\n",
    "The link function is given by:\n",
    "\\begin{align}\n",
    "P(y^{(i)}) = +1 | x^{(i)}, w) = \\frac{1}{1 + exp(-w^T x^{(i)})}\n",
    "\\end{align}\n",
    "where the feature vector $x^{(i)}$ represents the word counts of important_words in the review $x^{(i)}$.\n",
    "$P(y=+1|x,w)$ is the probability that sentiment of example 1 is 1 parametrized by  $w$:\n",
    "    \n",
    "\\begin{align}\n",
    "P(y=+1|x,w) = sigmoid(w^T x) = \\frac{1}{(1+exp(-w^T x))}\n",
    "\\end{align}\n",
    "\n",
    "Since the word counts are stored as columns in [feature\\_matrix], each i-th row of the matrix corresponds to the feature vector $x^{(i)}$:\n",
    "\\begin{align}\n",
    "[feature\\_matrix] = \\left[\\begin{matrix} (x^{(1)})^T \\\\(x^{(2)})^T\\\\.\\\\.\\\\.\\\\(x^{(m)})^T\\end{matrix} \\right] = \\left[\\begin{matrix} x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & .... & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & .... & x_n^{(2)} \\\\\n",
    ".&.&.&.&.\\\\\n",
    ".&.&.&.&.\\\\\n",
    "x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & .... & x_n^{(m)}\n",
    "\\end{matrix} \\right]\n",
    "\\end{align}\n",
    "The score vector is defined by:\n",
    "\\begin{align}\n",
    "[score] = [feature\\_matrix] * w = \\left[\\begin{matrix} (x^{(1)})^T \\\\(x^{(2)})^T\\\\.\\\\.\\\\.\\\\(x^{(m)})^T\\end{matrix} \\right] * \\left[ \\begin{matrix} w_0 \\\\w_1\\\\w_2\\\\.\\\\.\\\\.\\\\w_n \\end{matrix} \\right] = \\left[\\begin{matrix} x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & .... & x_n^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & .... & x_n^{(2)} \\\\\n",
    ".&.&.&.&.\\\\\n",
    ".&.&.&.&.\\\\\n",
    "x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & .... & x_n^{(m)}\n",
    "\\end{matrix} \\right] * \\left[ \\begin{matrix} w_0 \\\\w_1\\\\w_2\\\\.\\\\.\\\\.\\\\w_n \\end{matrix} \\right]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "[score] = \\left[\\begin{matrix} x_0^{(1)}*w_0 + x_1^{(1)} * w_1 + x_2^{(1)}*w_2 + .... + x_n^{(1)}*w_n \\\\\n",
    "x_0^{(2)}*w_0 + x_1^{(2)} * w_1 + x_2^{(2)}*w_2 + .... + x_n^{(2)}*w_n \\\\\n",
    "x_0^{(3)}*w_0 + x_1^{(3)} * w_1 + x_2^{(3)}*w_2 + .... + x_n^{(3)}*w_n \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x_0^{(m)}*w_0 + x_1^{(m)} * w_1 + x_2^{(m)}*w_2 + .... + x_n^{(m)}*w_n \\\\\n",
    "\\end{matrix} \\right]\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_probability(feature_matrix, coefficients):\n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    predictions = 1/(1 + np.exp(-score))\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute derivative of log-likelihood with respect to a single coefficient $w_j$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell}{ \\partial w_j} = \\sum_{i=1} ^m x_j^{(i)} * [errors]\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "[errors] = \\left(\\mathbb{1}[y^{(i)}=+1] - P(y^{(i)}=+1 | x^{(i)},w) \\right)\n",
    "\\end{align}\n",
    "The log likelihood simplifies the derivation of the gradient and is more numerically stable. \n",
    "\\begin{align}\n",
    "\\ell \\ell (w) = \\sum _{i=1} ^m \\left( (\\mathbb{1}[y^{(i)}=+1]-1)w^{T} x^{(i)} - ln(1+exp(-w^{T} x^{(i)})) \\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    derivative = np.dot(np.transpose(feature), errors)\n",
    "    return derivative\n",
    "\n",
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment == +1)    \n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((indicator - 1)*scores -np.log(1. + np.exp(-scores)))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logictic_regression() takes gradient steps toward optimum.\n",
    "The function accepts the following parameters:\n",
    "<ol>\n",
    "<li> [feature_matrix]: 2D array of features\n",
    "<li> [sentiment] : 1D array of class labels\n",
    "<li> [initial_coefficients]: 1D array containing initial values of coefficients\n",
    "<li> [step_size]: a parameter controlling the size of the gradient steps\n",
    "<li> [max_iter] : number of iterations to run gradient ascent.\n",
    "</ol>\n",
    "and returns the last set of coefficients after performing gradient ascent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients)\n",
    "    for itr in xrange(max_iter):\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        indicator = (sentiment == +1)\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): #loop over each coefficient\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,j])\n",
    "            coefficients[j] = coefficients[j] + step_size * derivative\n",
    "        \n",
    "        if itr<=15 or (itr <= 100 and itr%10 == 0) or (itr <= 1000 and itr%100 == 0) or (itr <= 10000 and itr%100 == 0) \\\n",
    "        or itr%10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' %(int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load review dataset in dataFrame. \n",
    "\n",
    "<ol> \n",
    "<li> IMPORT CSV FILE (TRAINING SET):\n",
    "The csv file has 4 cols:\n",
    "<ul>\n",
    "    <li> name of product = name,\n",
    "    <li> review (type='text'),\n",
    "    <li> rating (type=int $\\in$ {0,1,2,3,4,5}),\n",
    "    <li> sentiment (type=in $\\in$ {1(positive sentiment), -1(negative sentiment)}\n",
    "</ul>\n",
    "\n",
    "<li> APPLY TEXT CLEANING ON ALL REVIEWS: \n",
    "remove the punctuations and fills n/a values with empty string: ''\n",
    "<li> IMPORT important_words.json file:\n",
    "contains a list of 193 words most frequently used. This will be our feature.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbr of features: 193\n",
      "All of my kids have cried non-stop when I tried to ween them off their pacifier, until I found Thumbuddy To Love's Binky Fairy Puppet.  It is an easy way to work with your kids to allow them to understand where their pacifier is going and help them part from it.This is a must buy book, and a great gift for expecting parents!!  You will save them soo many headaches.Thanks for this book!  You all rock!!\n",
      "List of the first 10 products: 1       Nature's Lullabies Second Year Sticker Calendar\n",
      "2       Nature's Lullabies Second Year Sticker Calendar\n",
      "3                           Lamaze Peekaboo, I Love You\n",
      "4     SoftPlay Peek-A-Boo Where's Elmo A Children's ...\n",
      "5                             Our Baby Girl Memory Book\n",
      "6     Hunnt&reg; Falling Flowers and Birds Kids Nurs...\n",
      "7     Blessed By Pope Benedict XVI Divine Mercy Full...\n",
      "8     Cloth Diaper Pins Stainless Steel Traditional ...\n",
      "9     Cloth Diaper Pins Stainless Steel Traditional ...\n",
      "10    Newborn Baby Tracker&reg; - Round the Clock Ch...\n",
      "Name: name, dtype: object\n",
      "Generate review_clean column\n"
     ]
    }
   ],
   "source": [
    "dataFile = r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig2/amazon_baby_subset.csv'\n",
    "file_important_words = r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig2/important_words.json'\n",
    "important_words = json.load(open(file_important_words)) #open the json file as a string and parse it with json.load ==> a list\n",
    "nbr_features = len(important_words)\n",
    "print 'nbr of features:', nbr_features\n",
    "colNames = ['name', 'review', 'rating', 'sentiment']\n",
    "#products = pd.read_table(dataFile, sep=' ', header=None, names=colNames) # user_id gender  age  occupation    zip\n",
    "products = pd.read_csv(dataFile, header=0, names=colNames) #[shape=(183531,3)].\\n\"\n",
    "#load important_word json file\n",
    "print products['review'][0]\n",
    "#Name of the first 10 products\n",
    "print 'List of the first 10 products:', products['name'][1:11]  #must use iloc to return element at index id (products.iloc[1])\n",
    "\n",
    "print 'Generate review_clean column'\n",
    "#for the empty review, fill (n/a)\n",
    "products = products.fillna({'review':''})\n",
    "#Apply text cleaning to the data: create a new column with review without punctuations\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate feature columns for each review/training example\n",
    "\n",
    "<ol>\n",
    "<li> For each word in important_words (193 words), the number of times the word occurs in the review is reported in a column: 'contains_important_words[j]'.\n",
    "<li> print the number of product reviews that contain the word perfect\n",
    "</ol>\n",
    "\n",
    "Notes:\n",
    "<ul>\n",
    "<li> list.count(obj) counts occurence of obj in list\n",
    "<li> Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, using a construct called \"lambda\". The lambda definition does not include a \"return\" statement.\n",
    "For example: #def f (x): return x**2 is equivalent to g = lambda x: x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate columns w/ count in review of important_words\n",
      "Number of reviews with the word perfect : 2955\n"
     ]
    }
   ],
   "source": [
    "print 'Generate columns w/ count in review of important_words'\n",
    "for word in important_words:\n",
    "    products['contains_'+word] = products['review_clean'].apply(lambda s: s.split().count(word))\n",
    "#print 'this is product 0:', products.iloc[0]\n",
    "\n",
    "print 'Number of reviews with the word perfect :', products[products['contains_perfect']>0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate feature_matrix, sentiment array\n",
    "<ol>\n",
    "<li> Convert the dataFrame to a multidimentional array\n",
    "<li> initialize the coefficients $\\vec{w}$\n",
    "<li> print the number of features (including bias)\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature_matrix has <  194  > features, including the bias/intercept\n"
     ]
    }
   ],
   "source": [
    "dataFrame_arrays = dataFrame2Array(products, important_words, 'sentiment')\n",
    "feature_matrix = dataFrame_arrays[0] #item #9\n",
    "Nbr_of_examples = np.shape(feature_matrix)[0]\n",
    "sentiment = dataFrame_arrays[1]\n",
    "initial_coefficients = np.zeros((nbr_features+1,1))\n",
    "step_size = 1e-7\n",
    "max_iter = 301\n",
    "print 'The feature_matrix has < ', np.shape(feature_matrix)[1],' > features, including the bias/intercept'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run learning algorithm (logistic_regression)\n",
    "\n",
    "<ul>\n",
    "<li> The function [logistic_regression()] carries out the following steps:\n",
    "<ol>\n",
    "<li> initialize vector [coefficients] to [initial_coefficients]\n",
    "<li> Predict the class probability $P(y^{(i)}=+1|x^{(i)},w)$ using [predict_probability()] and save it to the variable [predictions]\n",
    "<li> Compute indicator value for $(y^{(i)}=+1)$ by comparing [sentiment] against +1. Save it to a variable [indicator].\n",
    "<li> Compute the errors as difference between [indicator] and [predictions], and save it to variable [errors]\n",
    "<li> For each $j-$th coeffcient, compute the per-coefficient derivative by calling [feature_derivative] with the $j-$th column of [feature_matrix]. Then increment the $j-$th coefficient by ([step_size]*[derivative]\n",
    "</ol>\n",
    "\n",
    "<li> Predicting sentiments\n",
    "<ol> \n",
    "<li> Class predictions for a data $x$ can be computed from the coefficients $w$ using the following formula:\n",
    "\\begin{align}\n",
    "y = \n",
    "\\begin{cases}\n",
    "    +1 & x^T w \\geq 0 \\\\\n",
    "    -1 & x^T w \\leq 0\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "<li> first compute the [scores] using [feature_matrix] and [coefficients] and a dot product\n",
    "<li> apply the threshold 0 on the [scores] to compute the class predictions.\n",
    "</ol>\n",
    "<li> Measure the accuracy of the algorithm:\n",
    "\n",
    "\\begin{align}\n",
    "accuracy = \\frac{\\text{# correctly classified data points}}{\\text{# total data points}}\n",
    "\\end{align}\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -36780.91768478\n",
      "iteration   1: log likelihood of observed labels = -36775.13434712\n",
      "iteration   2: log likelihood of observed labels = -36769.35713564\n",
      "iteration   3: log likelihood of observed labels = -36763.58603240\n",
      "iteration   4: log likelihood of observed labels = -36757.82101962\n",
      "iteration   5: log likelihood of observed labels = -36752.06207964\n",
      "iteration   6: log likelihood of observed labels = -36746.30919497\n",
      "iteration   7: log likelihood of observed labels = -36740.56234821\n",
      "iteration   8: log likelihood of observed labels = -36734.82152213\n",
      "iteration   9: log likelihood of observed labels = -36729.08669961\n",
      "iteration  10: log likelihood of observed labels = -36723.35786366\n",
      "iteration  11: log likelihood of observed labels = -36717.63499744\n",
      "iteration  12: log likelihood of observed labels = -36711.91808422\n",
      "iteration  13: log likelihood of observed labels = -36706.20710739\n",
      "iteration  14: log likelihood of observed labels = -36700.50205049\n",
      "iteration  15: log likelihood of observed labels = -36694.80289716\n",
      "iteration  20: log likelihood of observed labels = -36666.39512033\n",
      "iteration  30: log likelihood of observed labels = -36610.01327118\n",
      "iteration  40: log likelihood of observed labels = -36554.19728365\n",
      "iteration  50: log likelihood of observed labels = -36498.93316099\n",
      "iteration  60: log likelihood of observed labels = -36444.20783914\n",
      "iteration  70: log likelihood of observed labels = -36390.00909449\n",
      "iteration  80: log likelihood of observed labels = -36336.32546144\n",
      "iteration  90: log likelihood of observed labels = -36283.14615871\n",
      "iteration 100: log likelihood of observed labels = -36230.46102347\n",
      "iteration 200: log likelihood of observed labels = -35728.89418769\n",
      "iteration 300: log likelihood of observed labels = -35268.51212683\n",
      "How many reviews were predicted to have positive sentiment [25126]\n",
      "[39903]\n",
      "What is the accuracy of the model on predictions made above:  75.1865390413\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter)\n",
    "score = np.dot(feature_matrix, coefficients)\n",
    "score[score > 0] = 1\n",
    "score[score <= 0] = -1\n",
    "#Number of product reviews that contain the words perfect\n",
    "comparison = np.zeros((Nbr_of_examples,1))\n",
    "comparison = (sentiment==score)\n",
    "print 'How many reviews were predicted to have positive sentiment', sum(score > 0)\n",
    "print sum(comparison==1)\n",
    "accuracy = float(sum(comparison==1))/Nbr_of_examples*100\n",
    "print 'What is the accuracy of the model on predictions made above: ', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis: words contributing more to the positive and negative reviews\n",
    "<ul>\n",
    "<li> create a tuple (word,coefficient_value) \n",
    "    <ul>\n",
    "        <li> coefficients = list(coefficients[1:]) and,\n",
    "        <li> word[ : ]\n",
    "        <li> zip\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 words that have the most positive coeffcient values: [([0.0665460841704577], 'great'), ([0.06589076292212327], 'love'), ([0.0647945868025784], 'easy'), ([0.04543562630842137], 'little'), ([0.04497640139490604], 'loves'), ([0.030135001092107077], 'well'), ([0.02973993710496846], 'perfect'), ([0.020077541034775378], 'old'), ([0.018408707995268992], 'nice'), ([0.01770319990570169], 'daughter')]\n",
      "The 10 words that have the most negative coeffcient values: [([-0.053860148445203135], 'would'), ([-0.04151103339210889], 'product'), ([-0.038982037286487116], 'money'), ([-0.03306951529475273], 'work'), ([-0.030051249236035804], 'even'), ([-0.028978976142317068], 'disappointed'), ([-0.028711552980192574], 'get'), ([-0.027742697230661334], 'back'), ([-0.026592778462247283], 'return'), ([-0.024482100545891717], 'monitor')]\n"
     ]
    }
   ],
   "source": [
    "coefficients = coefficients.tolist()[1:]\n",
    "word_coefficient_tuple = sorted(zip(coefficients, important_words), key=lambda x:x[0], reverse=True)\n",
    "print 'The 10 words that have the most positive coeffcient values:', word_coefficient_tuple[0:10]\n",
    "\n",
    "word_coefficient_tuple = sorted(zip(coefficients, important_words), key=lambda x:x[0])\n",
    "print 'The 10 words that have the most negative coeffcient values:', word_coefficient_tuple[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
