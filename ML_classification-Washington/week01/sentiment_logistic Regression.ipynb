{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "### Introduction:\n",
    "The goal is to use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative [1]. The code includes:\n",
    "<ol>\n",
    "    <li><p>data cleaning</p></li>\n",
    "    <li><p>feature engineering</p></li>\n",
    "    <li><p>Train a logistic regression model to predict the sentiment of product reviews</p></li>\n",
    "    <ul>\n",
    "        <li>inspect the weights of a trained logistic regression model</li>\n",
    "        <li>make a prediction (for class and probability) of sentiment for a new product review</li>\n",
    "        <li>compute the accuracy of the model</li> \n",
    "    </ul>\n",
    "    <li>compare different logistic regression models</li>\n",
    "</ol>\n",
    "\n",
    "(1)This is one of the assignments from the Coursera class: [Machine Learning: Classification](https://www.coursera.org/learn/ml-classification/home/welcome)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><p>Import the required modules</p></li>\n",
    "    <li><p>create 2 functions:</p></li>\n",
    "    <ul>\n",
    "        <li>remove_ponctuations(): replace all ponctuations in the review by None.</li>\n",
    "        <li>json_2_np(): the list of samples (in fact list of indexes) to use as training examples or as test examples were given in a json file. Note that there is apparently a buil-in function to do that (will update that soon)</li>\n",
    "    <ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import exp\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(None, string.punctuation)                      \n",
    "\n",
    "def json_to_np(fileName):\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    content = ''.join(content)\n",
    "    content = content.translate(None,'[,]')\n",
    "    return np.fromstring(content, dtype=int, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Load amazon dataset (csv file) in a pandas DataFrame 'products' \n",
    "    The file has 3 columns: baby product name(type=text)/reviews(type=text)/rating (type=int(1-5))\n",
    "    dataFile = r'E:\\Data...' since the slashes are special characters, prefixing the string with a 'r' will prevent escape the whole string. \n",
    "    </li>\n",
    "    \n",
    "    <li><p>Perform text cleaning:</p></li>\n",
    "    <ul>\n",
    "        <li>use remove_ponctuations() on all reviews</li>\n",
    "        <li>fill n/a values in the review column with empty strings (if applicable). The n/a values indicate empty reviews. Use pandas fillna()</li>\n",
    "        <li>save all the clean reviews as another column in products</li>\n",
    "    </ul>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFile = r'E:\\DataScientist\\myNotebook\\ML_classification (Uni.Washington)\\amazon_baby.csv'\n",
    "TrainFile = r'E:\\DataScientist\\myNotebook\\ML_classification (Uni.Washington)\\module-2-assignment-train-idx.json'\n",
    "TestFile = r'E:\\DataScientist\\myNotebook\\ML_classification (Uni.Washington)\\module-2-assignment-test-idx.json'\n",
    "products = pd.read_csv(dataFile, header=0) #[shape=(183531,3)].\n",
    "products.fillna({'review': ''}, inplace=True)\n",
    "#create a new col 'review_clean' = copy of col review but without \n",
    "#all punctuations in text: note that it removes also punctuation of I'd, would've, hadn't\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### EXTRACT SENTIMENT ########\n",
    "products = #[166752,4]\n",
    "#ignore all reviews with rating=3, bcz they tend to have neutral sentiment.\n",
    "#products['sentiment'][(products.sentiment > 3)] = 1 thsi approach can generate some error warning\n",
    "#change rating to \\in {-1,1} : for rating<3 new_rating=-1, rating>3 new_rating=1\n",
    "#use this token pattern  to keep single-letter words \n",
    "#First, learn vocabulary from the training data and assign columns of words\n",
    "#Then, convert the training data into a sparse matrix train_matrix\n",
    "#4.2.3.7. Limitations of the Bag of Words representation\n",
    "#A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn’t account for potential misspellings or word derivations.\n",
    "#N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.\n",
    "#One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations.\n",
    "#For example, let’s say we’re dealing with a corpus of two documents: ['words', 'wprds']. The second document contains a misspelling of the word ‘words’ . A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however , would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:\n",
    "#The metacharacter \\b is an anchor like the caret and the dollar sign. It matches at a position that is called a \"word boundary\". This match is zero-length.\n",
    "#There are three different positions that qualify as word boundaries:\n",
    "#Before the first character in the string, if the first character is a word character.\n",
    "#After the last character in the string, if the last character is a word character.\n",
    "#Between two characters in the string, where one is a word character and the other is not a word character.\n",
    "Buidl the word count vector for each review.\n",
    "We will now compute the word count for each word that appears in the reviews. A vector consisting of word counts is often referred to as bag-of-word features. Since most words occur in only a few reviews, word count vectors are sparse. For this reason, sci-kit learn and many other tools use sparse matrices to store a collection of word vectors.\n",
    "Learn a vocabulary (set of all words) from the training data. only the words that show up in the training data will be considered for feature extraction. Compute the occurences of the words in each review and collect them into a row vector.\n",
    "Build a sparse matrix where each row is the word count vector for the corresponding review. \n",
    "#You are not supposed to do fit_transform on your test data, but only transform. Otherwise, you will get different vectorization than the one used during training.\n",
    "\n",
    "#The traceback holds the answer here: when you call X.toarray() at the end, it's converting a sparse matrix representation \n",
    "#to a dense representation. This means that instead of storing a constant amount of data for each word in each document, you're now \n",
    "#storing a value for all words over all documents. Use a small subset of data and then toarray to see the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " step: ****Generating Train_matrix and Test_Matrix****\n"
     ]
    }
   ],
   "source": [
    "\n",
    "products = products[products['rating'] !=3] \n",
    "train_dataIdx = json_to_np(TrainFile) # [shape=(133416,)]\n",
    "test_dataIdx = json_to_np(TestFile) #[shape=(33336,)]\n",
    "\n",
    "products['sentiment'] = products['rating']\n",
    "products.loc[products['sentiment'] < 3, 'sentiment'] = -1\n",
    "products.loc[products['sentiment'] > 3, 'sentiment'] = 1\n",
    "train_data = products.iloc[train_dataIdx]\n",
    "test_data = products.iloc[test_dataIdx]\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "\n",
    "print '\\n step: ****Generating Train_matrix and Test_Matrix****'\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to flatten y into a 1-D array, so that scikit-learn will properly understand it as the response variable.\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data.\n",
    "first create an instance of the logistic regression class.\n",
    "Then call the method fit to train the classifier. this model should use the sparse word count matrix (train matrix) as features and the column sentiment of train_data as the target. Use the default values for other parameters. Call this model sentiment_model.\n",
    "there should be over 100,000 coefficients in this sentiment_model. remembef that positive weights correspond to weights that cause positive sentiments, while negative weights correspond to negative sentiment. Calculate the number of positive (>=0 ) coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " step: Start Logistic Regression\n",
      "\n",
      " Total Number of weights:  121712 | Number of positive weights:  85707\n"
     ]
    }
   ],
   "source": [
    "sentiment = np.ravel(train_data['sentiment']) # flatten y into a 1-D array\n",
    " \n",
    "# instantiate a logistic regression model, and fit with X and y\n",
    "print '\\n step: Start Logistic Regression'\n",
    "LogReg = LogisticRegression()\n",
    "sentiment_model = LogReg.fit(train_matrix, sentiment)\n",
    "weight_matrix = sentiment_model.coef_\n",
    "weight_m, weight_n = weight_matrix.shape\n",
    "neg_weight_count = (weight_matrix < 0).sum()\n",
    "pos_weight_count = weight_n - neg_weight_count\n",
    "print \"\\n Total Number of weights: \", weight_n ,\"| Number of positive weights: \", pos_weight_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10. We will now make a class prediction for the sample_test_data. The sentiment_model should predict +1 if the sentiment is positive and -1 if the sentiment is negative. The score (also called margin) is defined by:\n",
    "\\begin{align}\n",
    "score^{(i)}= w^T x^{(i)}\n",
    "\\end{align}\n",
    "where x^{(i)} is the features for data point $i$. For each row, the score/margin is in the range (+inf/-inf).  \n",
    "#Now that a model is trained we can make predictiosns. Use a subset of test_data (take the 11th, 12th and 13th data points in the test data)\n",
    " \n",
    "#11. Predicting sentiment\n",
    "#These score can be used to make class predictions as follows : y=+1 if \\Theta^T x > 0 and y=-1 if Theta^Tx <= 0 \n",
    "\n",
    "\\begin{align}\n",
    "y^{(i)} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "        +1 & \\mathrm{\\ if \\ } w^T x^{(i)}>0 \\\\\n",
    "        -1 & \\mathrm{\\ if \\ } w^T x^{(i)} \\leq 0\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 121712 features per sample; expecting 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-7349bda28450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_test_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msample_test_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_test_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_test_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mclass_predict_testData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 249\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 121712 features per sample; expecting 20"
     ]
    }
   ],
   "source": [
    "sample_test_data = test_data.iloc[10:13] \n",
    "sample_test_matrix = vectorizer.transform(sample_test_data['review_clean'])\n",
    "scores = sentiment_model.decision_function(sample_test_matrix)\n",
    "class_predict_testData = np.zeros((3,1))\n",
    "for index, myscore in enumerate(scores):\n",
    "    if myscore > 0 : \n",
    "        class_predict_testData[index] = 1\n",
    "    else:\n",
    "        class_predict_testData[index] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Checkpoint: Make sure your class predictions match with the ones obtained from sentiment_model. The logistic regression classifier in scikit-learn comes with the predict function for this purpose:\n",
    "#print '\\n Predicted Label from sentiment_model for sample_test_data 11/12/13: ', model_predict_spleTest    \n",
    "\n",
    "#12. Probability predictions\n",
    "\\begin{align}\n",
    "P(y^{(i)}=(+1|x^{(i)},w) = \\frac{1}{1+exp(-w^T x^{(i)})}\n",
    "\\end{align}\n",
    "\n",
    "Find the most positive and negative review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** P(y=1|x,w) (probability of sample test being pos. class (+1)): \n",
      "  [[  9.96300679e-01]\n",
      " [  4.17571851e-02]\n",
      " [  3.01815820e-05]]\n"
     ]
    }
   ],
   "source": [
    "model_predict_spleTest = sentiment_model.predict(sample_test_matrix)\n",
    "\n",
    "prob_pos_class = np.zeros((3,1))\n",
    " \n",
    "for index, myscore in enumerate(scores):\n",
    "    prob_pos = 1/(1 + exp(-myscore) ) \n",
    "    prob_pos_class[index] = prob_pos\n",
    "print '\\n*** P(y=1|x,w) (probability of sample test being pos. class (+1)): \\n ', prob_pos_class\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CheckPoint\n",
    "#print 'predict_proba output', model_proba_spleTest\n",
    "#you can define a custom function that takes a float value as its input and returns a formatted string:\n",
    "#The f here means fixed-point format (not 'scientific'), and the .2 means two decimal places (you can read \n",
    "#more about string formatting here).\n",
    "#float_formatter = lambda x: \"%.2f\" % x\n",
    "#model_proba = np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "#print '\\n P(y=1|x,w) (Model based probability of sample test being positive class(-1|+1)): ', model_proba_spleTest\n",
    "#model_proba_spleTest_arr = model_proba_spleTest_arr.reshape((model_proba_spleTest_NbrRows,1))\n",
    "#print model_proba_spleTest_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_proba_spleTest = sentiment_model.predict_proba(sample_test_matrix)\n",
    "model_proba_spleTest_arr = model_proba_spleTest[:,1]\n",
    "sample_test_data.loc[:,'probaPosRev'] = pd.Series(model_proba_spleTest_arr, index=sample_test_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#13. Examine full test dataset: test_data\n",
    "#Sort the dataframe's rows by reports, in ascending order\n",
    "##print '\\n*** Products of Top 20 reviews: ', top20reviews.iloc[0:20]['name']\n",
    "#14.\n",
    "##print '\\n*** Products of Bottom 20 reviews: ', bottom20reviews.iloc[0:20]['name']\n",
    "\n",
    "#15 Accuracy classifier on the train_data. Examine full train dataset: train_data:\n",
    "\n",
    "\\begin{align}\n",
    "accuracy = \\frac{\\mathrm{\\# \\  correctly \\  classified \\ examples}}{\\mathrm{\\# \\ total \\ examples}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 121712 features per sample; expecting 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-25351b1b16c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_proba_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_proba_test_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_proba_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'probaPosRev'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_proba_test_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtop20reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'probaPosRev'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\logistic.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[0mcalculate_ovr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ovr\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcalculate_ovr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\base.pyc\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \"\"\"\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\linear_model\\base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 249\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 121712 features per sample; expecting 20"
     ]
    }
   ],
   "source": [
    "model_proba_test = sentiment_model.predict_proba(test_matrix)\n",
    "model_proba_test_arr = model_proba_test[:,1]\n",
    "test_data.loc[:,'probaPosRev'] = pd.Series(model_proba_test_arr, index=test_data.index)\n",
    "\n",
    "top20reviews = test_data.sort_index(by='probaPosRev', ascending=0)\n",
    "\n",
    "bottom20reviews = test_data.sort_index(by='probaPosRev', ascending=1)\n",
    "\n",
    "model_sentiment_Train = sentiment_model.predict(train_matrix)\n",
    "train_data.loc[:,'modelSentiment'] = pd.Series(model_sentiment_Train, index=train_data.index)\n",
    "train_data.loc[:,'modelSentimentVSMeasured'] = pd.Series(train_data['modelSentiment'] == train_data['sentiment'], \\\n",
    "                                                         index=train_data.index )\n",
    "correctly_classified_TrainExples = float(len(train_data[train_data['modelSentimentVSMeasured']==True]))\n",
    "total_TrainExamples = train_data['modelSentimentVSMeasured'].shape[0]\n",
    "accuracy_sentiment_Train = (float(correctly_classified_TrainExples) / total_TrainExamples)\n",
    "\n",
    "print '\\n***The accuracy of the \"Sentiment model\" classifier on the training set is:  ', accuracy_sentiment_Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#15 Accuracy classifier on the test_data. Examine full test dataset: test_data\n",
    "#13. Examine full test dataset: test_data\n",
    "#Find the 20 reviews in the entire test_data with the highest probability of being classified as a positive review.\n",
    "#model_proba_Test = sentiment_model.predict_proba(test_matrix)\n",
    "#print model_proba_Test[1]\n",
    "# check the accuracy on the training set\n",
    "#sentiment_model.score(train_matrix, sentiment)\n",
    " \n",
    "#loc works on labels in the index.\n",
    "#iloc works on the positions in the index (so it only takes integers).\n",
    "#ix usually tries to behave like loc but falls back to behaving like iloc if the label is not in the index.\n",
    "#It's important to note some subtleties that can make ix slightly tricky to use:\n",
    " \n",
    "#if the index is of integer type, ix will only use label-based indexing and not fall back to position-based indexing. If the label is not in the index, an error is raised.\n",
    "#if the index does not contain only integers, then given an integer, ix will immediately use position-based indexing rather than label-based indexing. If however ix is given another type (e.g. a string), it can use label-based indexing.\n",
    "#Compute a new set\n",
    "   \n",
    "#17-Train logistic regression\n",
    "#We also need to flatten y into a 1-D array, so that scikit-learn will properly understand it as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the sentiment_model classifier on the test data is:  0.932325413967\n"
     ]
    }
   ],
   "source": [
    "model_sentiment_Test = sentiment_model.predict(test_matrix)\n",
    "test_data.loc[:,'modelSentiment'] = pd.Series(model_sentiment_Test, index=test_data.index)\n",
    "test_data.loc[:,'modelSentimentVSMeasured'] = pd.Series((test_data['modelSentiment'] == test_data['sentiment']), \\\n",
    "                                                        index=test_data.index )\n",
    "correctly_classified_TestExples = float(len(test_data[test_data['modelSentimentVSMeasured']==True]))\n",
    "total_TestExamples = float(test_data['modelSentimentVSMeasured'].shape[0])\n",
    "accuracy_testModelSentiment = (correctly_classified_TestExples/total_TestExamples)\n",
    "print 'The accuracy of the sentiment_model classifier on the test data is: ', accuracy_testModelSentiment\n",
    "\n",
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', 'well', 'able', 'car', \\\n",
    "                     'broke', 'less', 'even', 'waste', 'disappointed', 'work', 'product', 'money', 'would', 'return']\n",
    "\n",
    "vectorizer_word_subset = CountVectorizer(vocabulary=significant_words) #limit of 20 words\n",
    "train_matrix_word_subset = vectorizer_word_subset.fit_transform(train_data['review_clean'])\n",
    "test_matrix_word_subset = vectorizer_word_subset.fit_transform(test_data['review_clean'])\n",
    "\n",
    "simple_model = LogReg.fit(train_matrix_word_subset, sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "Build a table to store (word, coefficient) pairs. Consider the coefficients of the Simple Model, how many of the 20 coefficients (corresponding to the 20 significant words) are positive for the simple model. (Make sure to exclude the intercept term)\n",
    "\n",
    "## Comparing models\n",
    "   #accuracy of the sentiment model on the train_data\n",
    "\n",
    "\n",
    "On both, the training set and the test set, we compare the accuracy of the sentiment model vs that of the simple_Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Total Number of weights of the Simple Model that are positive:  10\n",
      "Accuracy of the Simple model on the training set:  0.866822570007  |Sentiment_Model:  0.967627570906\n",
      "Accuracy of the Simple model on the test set :  0.869360451164 | Sentiment_Model:  0.932325413967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\ipykernel\\__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "simple_model_coef_table = pd.DataFrame({'word':significant_words, 'coefficient': simple_model.coef_.flatten()})\n",
    "smallestWeights = simple_model_coef_table.sort_index(by='coefficient', ascending=0)\n",
    "   \n",
    "weight_m, weight_n = simple_model.coef_.shape\n",
    "neg_weight_count = (simple_model.coef_ < 0).sum()\n",
    "pos_weight_count = weight_n - neg_weight_count\n",
    "print \"\\n***Total Number of weights of the Simple Model that are positive: \", pos_weight_count\n",
    "\n",
    "model_simple_Train = simple_model.predict(train_matrix_word_subset)\n",
    "train_data.loc[:,'simpleModel'] = pd.Series(model_simple_Train, index=train_data.index)\n",
    "train_data.loc[:,'SimpleModelVSMeasured'] = pd.Series((train_data['simpleModel'] == train_data['sentiment']), \\\n",
    "                                                      index=train_data.index) \n",
    "pos_classified_TrainExples_sm = float(train_data['SimpleModelVSMeasured'].sum())\n",
    "accuracy_simpleModel_Train = pos_classified_TrainExples_sm / total_TrainExamples\n",
    "print 'Accuracy of the Simple model on the training set: ', accuracy_simpleModel_Train , ' |Sentiment_Model: ', \\\n",
    "accuracy_sentiment_Train\n",
    "\n",
    "model_simple_Test = simple_model.predict(test_matrix_word_subset)\n",
    "test_data.loc[:,'simpleModel'] = pd.Series(model_simple_Test, index=test_data.index)\n",
    "test_data['SimpleModelVSMeasured'] = np.array(test_data['simpleModel'] == test_data['sentiment'] )\n",
    "pos_classified_TestExples_sm = float(test_data['SimpleModelVSMeasured'].sum())\n",
    "accuracy_simpleModel_Test = (pos_classified_TestExples_sm / total_TestExamples)\n",
    "print 'Accuracy of the Simple model on the test set : ', accuracy_simpleModel_Test, '| Sentiment_Model: ', \\\n",
    "accuracy_testModelSentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Majority Class Prediction\n",
    "The majority classifier is a very simple model, which predicts the majority output label in training data for all results.\n",
    "This can be used as a reference model for comparison with the classifier model. Let's say I have a training data set of size 100. 60 are positives (+1) and 40 are negatives (-1). Now since positives are majority, the model will predict positive for any input.\n",
    "#Let's assume we have a test data set of size 50. 10 are positives, and 40 negatives. Since your model predicts positive for all of them, it is correct only 10 times. Your accuracy is then 10/50 = 0.2\n",
    "#If you have a test set of size 50 with 32 positives and 18 negatives, your accuracy will be 32/50 = 0.64\n",
    "#I hope this helps.\n",
    "<ol>\n",
    "    <li>Find the majority of the class in training data ('1' or '-1'), i.e the class with the largest count.<li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Accuracy of the majority Class Classifier: 0.842782577394\n"
     ]
    }
   ],
   "source": [
    "#occurence of values in column sentiment ordered by ascendant: this is a series.\n",
    "getClassCnts_sentiment_train = pd.value_counts(train_data['sentiment']) \n",
    "majorClass_in_train = getClassCnts_sentiment_train.keys()[0]\n",
    "countsMaj_test = test_data[test_data.sentiment == majorClass_in_train].sum()['sentiment']\n",
    "majority_class_accuracy = float(countsMaj_test)/float(total_TestExamples)\n",
    "print '\\n*** Accuracy of the majority Class Classifier:', majority_class_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
