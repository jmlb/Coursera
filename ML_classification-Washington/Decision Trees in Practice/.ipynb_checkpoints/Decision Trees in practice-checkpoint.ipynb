{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees in Practice (not working)\n",
    "In this assignment we will explore various tec hniques for preventing overfitting in decision trees. We will extend the implementation of the binary decision trees that we implemented in the previous assignment. You will have to use your solutions from this previous assignment and extend them.\n",
    "\n",
    "In this assignment you will:\n",
    "\n",
    "* Implement binary decision trees with different early stopping methods.\n",
    "* Compare models with different stopping parameters.\n",
    "* Visualize the concept of overfitting in decision trees.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using 25 features in the model.\n"
     ]
    }
   ],
   "source": [
    "dataFile = r'lending-club-data.csv'\n",
    "#1. Load in the LendingClub dataset \n",
    "loans = pd.read_csv(dataFile, header=0, low_memory=False)\n",
    "#2. Reassign the labels to have +1 for a safe loan, and -1 for a risky (bad) loan.\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "#delete column 'bad_loans'\n",
    "loans = loans.drop('bad_loans', 1)\n",
    "#3. We will be using the following features:\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "#Extract these feature columns from the dataset, and discard the rest of the feature columns.\n",
    "loans = loans[features + [target]]\n",
    "#name the new features to keep the core name: for example grade_A, etc...\n",
    "for colName in features:\n",
    "    if loans[colName].dtype == object :\n",
    "        # Create a set of dummy variables from the sex variable\n",
    "        dummies = pd.get_dummies(loans[colName])\n",
    "        #update dummies cols name to include initial name reference\n",
    "        for dummiesName in dummies.columns.values:\n",
    "            newName = colName + '_' + dummiesName\n",
    "            dummies.rename(columns={dummiesName: newName}, inplace=True)\n",
    "        # Join the dummy variables to the main dataframe\n",
    "        loans = loans.join(dummies)\n",
    "        loans = loans.drop(colName, 1)       \n",
    "features = loans.columns.values\n",
    "features = np.setdiff1d(features, target)\n",
    " \n",
    "print('We will be using %d features in the model.' %(len(features)))\n",
    "#Load list of indices for the training and validation sets\n",
    "#open the json file as a string and parse it with json.load ==> a list\n",
    "train_idx = json.load(open(r'module-6-assignment-train-idx.json')) \n",
    "validation_idx = json.load(open(r'module-6-assignment-validation-idx.json'))\n",
    "train_data = loans.iloc[train_idx]\n",
    "validation_set = loans.iloc[validation_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Early Stopping Methods\n",
    "\n",
    "#Early stopping condition 1: Maximum depth\n",
    "#This is already implemented the maximum depth stopping condition in the main function.\n",
    "\n",
    "#Early stopping condition 2: minimum node size (set by parameter min_node_size)\n",
    "#The function takes 2 arguments: the data (from a node) and the minimum number of data points that a node is \n",
    "# allowed to split on, min_node_size\n",
    "#This function simply calculates whether the number of data points at a given node is less than or equal to the \n",
    "#specified minimum node size. This function will be used to detect this early stopping condition in the \n",
    "#decision_tree_create function \n",
    "def reached_minimum_node_size(data, min_node_size):\n",
    "    #return True if the number of data points is less than or equal to the minimum node.size\n",
    "    return (data.shape[0] <= min_node_size)\n",
    "\n",
    "#Early stopping condition 3: minimum gain in error reduction\n",
    "#The function error_reduction takes 2 arguments: error before a split (error_before_split) and error after a split \n",
    "#(error_after_split)\n",
    "# This function computes the gain in error reduction, i.e., the difference between the error before the split \n",
    "#and that after the split. This function will be used to detect this early stopping condition in the \n",
    "#decision_tree_create function.\n",
    "\n",
    "def error_reduction(error_before_split, error_after_split):\n",
    "    return error_before_split - error_after_split\n",
    "\n",
    "#Calculates number of misclassified examples when predicting the majority class.\n",
    "#This is used to help determine which feature is best to split on at a given node of the tree\n",
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    #labels_in_node is 'safe_loans' column \n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if labels_in_node.shape[0] == 0:\n",
    "        return 0    \n",
    "    # Count the number of 1's (safe loans)\n",
    "    count_safe_loans = sum(labels_in_node[labels_in_node == 1])\n",
    "    # Count the number of -1's (risky loans)\n",
    "    count_risky_loans = -sum(labels_in_node[labels_in_node == -1])             \n",
    "    # Return the number of mistakes that the majority classifier makes.\n",
    "    return min(count_safe_loans, count_risky_loans)\n",
    "\n",
    "#finds the best feature to split on given the data and a list of features to consider.\n",
    "def best_splitting_feature(data, features_list, target):\n",
    "    target_values = data[target]\n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_error = 10 # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(data.shape[0])\n",
    "    #loop thru each feature to consider splitting on that feature\n",
    "    for feature in features_list:\n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        left_split = data[data[feature]==0]\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        right_split = data[data[feature]==1]\n",
    "        # Calculate the number of misclassified examples in the left/right split.\n",
    "        #using the function intermediate_node_num_mistakes()\n",
    "        left_split_mistakes = intermediate_node_num_mistakes(left_split[target])\n",
    "        right_split_mistakes = intermediate_node_num_mistakes(right_split[target])       \n",
    "        # Compute the classification error of this split.\n",
    "        # Error = (# of mistakes (left) + # of mistakes (right)) / (# of data points)\n",
    "        error =(left_split_mistakes + right_split_mistakes)/num_data_points\n",
    "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
    "        ## YOUR CODE HERE\n",
    "        if error < best_error: \n",
    "            best_error = error\n",
    "            best_feature = feature            \n",
    "    return best_feature #Return the best feature, feature split that gives the lowest error \n",
    "\n",
    "#Creates a leaf node given a set of target values.\n",
    "def create_leaf(target_values):    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,  \n",
    "            'prediction': None }\n",
    "   \n",
    "    # Count the number of data points that are +1 and -1 in this node.\n",
    "    num_ones = sum(target_values[target_values == +1])\n",
    "    num_minus_ones = -sum(target_values[target_values == -1])  \n",
    "\n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = +1\n",
    "    else:\n",
    "        leaf['prediction'] = -1       \n",
    "\n",
    "    # Return the leaf node\n",
    "    return leaf\n",
    "\n",
    "#Incorporating new early stopping conditions in binary decision tree implementation\n",
    "\n",
    "def decision_tree_create(data, features, target, current_depth = 0, max_depth = 10, min_node_size = 1, \\\n",
    "                         min_error_reduction = 0.0):\n",
    "    \n",
    "    remaining_features = features[:] # features remaining to be splitted\n",
    "    target_values = data[target] #loans = +/-1\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "\n",
    "    # Stopping condition 1: Check if all nodes are of the same type (no mistakes)\n",
    "\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:\n",
    "        print(\"Stopping condition 1 reached. All data points have the same target value\")     \n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Stopping condition 2: No more features to split on\n",
    "    if remaining_features == []:  \n",
    "        print(\"Stopping condition 2 reached. No remaining features\")  \n",
    "        return create_leaf(target_values)    \n",
    "\n",
    "    # Early stopping condition 1 : Reached maximum depth limit \n",
    "    if current_depth >= max_depth:\n",
    "        print(\"Early stopping condition 1 reached. Reached maximum depth\")\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "    # Early stopping condition 2: Reached the minimum node size.\n",
    "    # If the number of data points is less than or equal to the minimum size, return a leaf.\n",
    "    if reached_minimum_node_size(data, min_node_size):           ## YOUR CODE HERE \n",
    "        print(\"Early stopping condition 2 reached. Reached minimum node size.\")\n",
    "        return create_leaf(target_values)\n",
    "    # Find the best splitting feature\n",
    "    splitting_feature = best_splitting_feature(data, remaining_features, target)\n",
    "    # Split on the best feature that we found. \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    remaining_features = np.setdiff1d(remaining_features, splitting_feature)\n",
    "    #print(\"Split on feature %s. (%s, %s)\" % (splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Early stopping condition 3: Minimum error reduction\n",
    "    # Calculate the error before splitting (number of misclassified examples divided by the total number of examples)\n",
    "    error_before_split = intermediate_node_num_mistakes(target_values) / float(data.shape[0])\n",
    "    \n",
    "    # Calculate the error after splitting (number of misclassified example in both groups divided by the \n",
    "    #total number of examples)\n",
    "    left_mistakes = min(len(left_split[left_split[target]==1]), len(left_split[left_split[target]==-1]))\n",
    "    print(left_mistakes)\n",
    "    right_mistakes = min(len(right_split[right_split[target]==1]), len(right_split[right_split[target]==-1]))\n",
    "    error_after_split = (left_mistakes + right_mistakes) / float(len(data))\n",
    "    # If the error reduction is LESS THAN OR EQUAL TO min_error_reduction, return a leaf.\n",
    "    if error_reduction(error_before_split, error_after_split) < min_error_reduction:\n",
    "        print(\"Early stopping condition 3 reached. Minimum error reduction.\")\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    remaining_features = np.setdiff1d(remaining_features, splitting_feature)\n",
    "    print(\"Split on feature %s. (%s, %s)\" % (splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, \n",
    "                                     current_depth + 1, max_depth, min_node_size, min_error_reduction)        \n",
    "    \n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, current_depth + 1, \\\n",
    "                                      max_depth, min_node_size, min_error_reduction)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}\n",
    "\n",
    "\n",
    "#Build the tree!\n",
    "def classify(tree, x, annotate = False):\n",
    "       # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate:\n",
    "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction']\n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate:\n",
    "             print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)\n",
    "\n",
    "def evaluate_classification_error(tree, data, target):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x, annotate=True), axis=1) \n",
    "    #needs axis to apply to each row, per default columns\n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    data['prediction'] = prediction\n",
    "    data['mistakes'] = data.apply(lambda x : 0 if x['prediction'] == x[target] else 1, axis=1)\n",
    "    Nbr_errors = sum(data['mistakes'])\n",
    "    \n",
    "    return (Nbr_errors*1.0/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ train data ************\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmlbeaujour/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:108: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3221\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "3159\n",
      "Split on feature grade_A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "2985\n",
      "Split on feature emp_length_1 year. (8602, 520)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8602 data points).\n",
      "1865\n",
      "Split on feature emp_length_10+ years. (5518, 3084)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5518 data points).\n",
      "1592\n",
      "Split on feature emp_length_2 years. (4755, 763)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (4755 data points).\n",
      "1359\n",
      "Split on feature emp_length_3 years. (4081, 674)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (4081 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (674 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (763 data points).\n",
      "273\n",
      "Split on feature emp_length_3 years. (763, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (763 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (3084 data points).\n",
      "1120\n",
      "Split on feature emp_length_2 years. (3084, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3084 data points).\n",
      "1120\n",
      "Split on feature emp_length_3 years. (3084, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3084 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (520 data points).\n",
      "174\n",
      "Split on feature emp_length_10+ years. (520, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (520 data points).\n",
      "174\n",
      "Split on feature emp_length_2 years. (520, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (520 data points).\n",
      "174\n",
      "Split on feature emp_length_3 years. (520, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (520 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "35\n",
      "Split on feature emp_length_n/a. (96, 5)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (96 data points).\n",
      "Early stopping condition 2 reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (5 data points).\n",
      "Early stopping condition 2 reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "9733\n",
      "Split on feature grade_D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23300 data points).\n",
      "8934\n",
      "Split on feature grade_E. (22024, 1276)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (22024 data points).\n",
      "8684\n",
      "Split on feature grade_F. (21666, 358)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (21666 data points).\n",
      "8168\n",
      "Split on feature emp_length_n/a. (20734, 932)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (20734 data points).\n",
      "8098\n",
      "Split on feature grade_G. (20638, 96)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (20638 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (96 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (932 data points).\n",
      "273\n",
      "Split on feature grade_A. (702, 230)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (702 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (230 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (358 data points).\n",
      "101\n",
      "Split on feature emp_length_8 years. (347, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (347 data points).\n",
      "93\n",
      "Split on feature emp_length_1 year. (313, 34)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (313 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (34 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Early stopping condition 2 reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1276 data points).\n",
      "435\n",
      "Split on feature emp_length_1 year. (1173, 103)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1173 data points).\n",
      "320\n",
      "Split on feature emp_length_10+ years. (872, 301)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (872 data points).\n",
      "262\n",
      "Split on feature emp_length_2 years. (720, 152)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (720 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (152 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (301 data points).\n",
      "115\n",
      "Split on feature emp_length_2 years. (301, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (301 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (103 data points).\n",
      "42\n",
      "Split on feature emp_length_10+ years. (103, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (103 data points).\n",
      "42\n",
      "Split on feature emp_length_2 years. (103, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (103 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4701 data points).\n",
      "1816\n",
      "Split on feature emp_length_1 year. (4342, 359)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4342 data points).\n",
      "1312\n",
      "Split on feature emp_length_10+ years. (3173, 1169)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (3173 data points).\n",
      "1107\n",
      "Split on feature emp_length_2 years. (2694, 479)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2694 data points).\n",
      "925\n",
      "Split on feature emp_length_3 years. (2267, 427)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2267 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (427 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (479 data points).\n",
      "205\n",
      "Split on feature emp_length_3 years. (479, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (479 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1169 data points).\n",
      "504\n",
      "Split on feature emp_length_2 years. (1169, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1169 data points).\n",
      "504\n",
      "Split on feature emp_length_3 years. (1169, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1169 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (359 data points).\n",
      "144\n",
      "Split on feature emp_length_10+ years. (359, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (359 data points).\n",
      "144\n",
      "Split on feature emp_length_2 years. (359, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (359 data points).\n",
      "144\n",
      "Split on feature emp_length_3 years. (359, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (359 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "----------------------------------------------\n",
      "safe_loans                -1.0\n",
      "grade_A                    0.0\n",
      "grade_B                    0.0\n",
      "grade_C                    0.0\n",
      "grade_D                    1.0\n",
      "grade_E                    0.0\n",
      "grade_F                    0.0\n",
      "grade_G                    0.0\n",
      "term_ 36 months            0.0\n",
      "term_ 60 months            1.0\n",
      "home_ownership_MORTGAGE    0.0\n",
      "home_ownership_OTHER       0.0\n",
      "home_ownership_OWN         0.0\n",
      "home_ownership_RENT        1.0\n",
      "emp_length_1 year          0.0\n",
      "emp_length_10+ years       0.0\n",
      "emp_length_2 years         1.0\n",
      "emp_length_3 years         0.0\n",
      "emp_length_4 years         0.0\n",
      "emp_length_5 years         0.0\n",
      "emp_length_6 years         0.0\n",
      "emp_length_7 years         0.0\n",
      "emp_length_8 years         0.0\n",
      "emp_length_9 years         0.0\n",
      "emp_length_< 1 year        0.0\n",
      "emp_length_n/a             0.0\n",
      "Name: 24, dtype: float64\n",
      "Split on term_ 36 months = 0.0\n",
      "Split on grade_A = 0.0\n",
      "Split on emp_length_1 year = 0.0\n",
      "Split on emp_length_10+ years = 0.0\n",
      "Split on emp_length_2 years = 1.0\n",
      "Split on emp_length_3 years = 0.0\n",
      "At leaf, predicting -1\n",
      "Predicted class: -1 \n"
     ]
    }
   ],
   "source": [
    "#print(train_data.iloc[0])\n",
    "#14. Now, let's consider the first example of the test set and see what my_decision_tree model predicts \n",
    "#for this data point.\n",
    "print('************ train data ************')\n",
    "\n",
    "my_decision_tree_new = decision_tree_create(train_data, features, target, max_depth=6, min_node_size=100, \\\n",
    "                                            min_error_reduction=0.0)\n",
    "print('----------------------------------------------')\n",
    "#16. Now, let's consider the first example of the validation set and see what the \n",
    "# my_decision_tree_new model predicts for this data point. Your code should be analogous to\n",
    "print(validation_set.iloc[0])\n",
    "print('Predicted class: %s ' % classify(my_decision_tree_new, validation_set.iloc[0], annotate=True))\n",
    "\n",
    "#17. Let's add some annotations to our prediction to see what the prediction path was that lead to this \n",
    "#predicted class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmlbeaujour/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:108: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3221\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "3159\n",
      "Split on feature grade_A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "2985\n",
      "Split on feature emp_length_1 year. (8602, 520)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8602 data points).\n",
      "1865\n",
      "Split on feature emp_length_10+ years. (5518, 3084)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5518 data points).\n",
      "1592\n",
      "Split on feature emp_length_2 years. (4755, 763)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (4755 data points).\n",
      "1359\n",
      "Split on feature emp_length_3 years. (4081, 674)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (4081 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (674 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (763 data points).\n",
      "273\n",
      "Split on feature emp_length_3 years. (763, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (763 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (3084 data points).\n",
      "1120\n",
      "Split on feature emp_length_2 years. (3084, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3084 data points).\n",
      "1120\n",
      "Split on feature emp_length_3 years. (3084, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3084 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (520 data points).\n",
      "174\n",
      "Split on feature emp_length_10+ years. (520, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (520 data points).\n",
      "174\n",
      "Split on feature emp_length_2 years. (520, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (520 data points).\n",
      "174\n",
      "Split on feature emp_length_3 years. (520, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (520 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "35\n",
      "Split on feature emp_length_n/a. (96, 5)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (96 data points).\n",
      "29\n",
      "Split on feature emp_length_< 1 year. (85, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (85 data points).\n",
      "27\n",
      "Split on feature emp_length_1 year. (81, 4)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (81 data points).\n",
      "15\n",
      "Split on feature emp_length_10+ years. (50, 31)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (50 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (31 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (4 data points).\n",
      "2\n",
      "Split on feature emp_length_10+ years. (4, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (4 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (11 data points).\n",
      "5\n",
      "Split on feature emp_length_1 year. (11, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "5\n",
      "Split on feature emp_length_10+ years. (11, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (11 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (5 data points).\n",
      "1\n",
      "Split on feature emp_length_1 year. (5, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5 data points).\n",
      "1\n",
      "Split on feature emp_length_10+ years. (5, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (5 data points).\n",
      "1\n",
      "Split on feature emp_length_2 years. (5, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "9733\n",
      "Split on feature grade_D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23300 data points).\n",
      "8934\n",
      "Split on feature grade_E. (22024, 1276)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (22024 data points).\n",
      "8684\n",
      "Split on feature grade_F. (21666, 358)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (21666 data points).\n",
      "8168\n",
      "Split on feature emp_length_n/a. (20734, 932)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (20734 data points).\n",
      "8098\n",
      "Split on feature grade_G. (20638, 96)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (20638 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (96 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (932 data points).\n",
      "273\n",
      "Split on feature grade_A. (702, 230)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (702 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (230 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (358 data points).\n",
      "101\n",
      "Split on feature emp_length_8 years. (347, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (347 data points).\n",
      "93\n",
      "Split on feature emp_length_1 year. (313, 34)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (313 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (34 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "2\n",
      "Split on feature home_ownership_OWN. (9, 2)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (9 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1276 data points).\n",
      "435\n",
      "Split on feature emp_length_1 year. (1173, 103)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1173 data points).\n",
      "320\n",
      "Split on feature emp_length_10+ years. (872, 301)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (872 data points).\n",
      "262\n",
      "Split on feature emp_length_2 years. (720, 152)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (720 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (152 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (301 data points).\n",
      "115\n",
      "Split on feature emp_length_2 years. (301, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (301 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (103 data points).\n",
      "42\n",
      "Split on feature emp_length_10+ years. (103, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (103 data points).\n",
      "42\n",
      "Split on feature emp_length_2 years. (103, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (103 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4701 data points).\n",
      "1816\n",
      "Split on feature emp_length_1 year. (4342, 359)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4342 data points).\n",
      "1312\n",
      "Split on feature emp_length_10+ years. (3173, 1169)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (3173 data points).\n",
      "1107\n",
      "Split on feature emp_length_2 years. (2694, 479)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2694 data points).\n",
      "925\n",
      "Split on feature emp_length_3 years. (2267, 427)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2267 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (427 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (479 data points).\n",
      "205\n",
      "Split on feature emp_length_3 years. (479, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (479 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1169 data points).\n",
      "504\n",
      "Split on feature emp_length_2 years. (1169, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1169 data points).\n",
      "504\n",
      "Split on feature emp_length_3 years. (1169, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1169 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (359 data points).\n",
      "144\n",
      "Split on feature emp_length_10+ years. (359, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (359 data points).\n",
      "144\n",
      "Split on feature emp_length_2 years. (359, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (359 data points).\n",
      "144\n",
      "Split on feature emp_length_3 years. (359, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (359 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value\n",
      "Split on term_ 36 months = 0.0\n",
      "Split on grade_A = 0.0\n",
      "Split on emp_length_1 year = 0.0\n",
      "Split on emp_length_10+ years = 0.0\n",
      "Split on emp_length_2 years = 1.0\n",
      "Split on emp_length_3 years = 0.0\n",
      "At leaf, predicting -1\n",
      "Predicted class: -1 \n"
     ]
    }
   ],
   "source": [
    "my_decision_tree_old = decision_tree_create(train_data, features, target, max_depth=6, min_node_size=0, \\\n",
    "                                            min_error_reduction=-1)\n",
    "print('Predicted class: %s ' % classify(my_decision_tree_old, validation_set.iloc[0],annotate = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: For my_decision_tree_new trained with max_depth = 6, min_node_size = 100, min_error_reduction=0.0, \n",
    "    is the prediction path for validation_set[0] shorter, longer, or the same as for my_decision_tree_old that \n",
    "    ignored the early stopping conditions 2 and 3?\n",
    "\n",
    "Quiz question: For my_decision_tree_new trained with max_depth = 6, min_node_size = 100, min_error_reduction=0.0, \n",
    "    is the prediction path for any point always shorter, always longer, always the same, shorter or the same, or \n",
    "    longer or the same as for my_decision_tree_old that ignored the early stopping conditions 2 and 3?\n",
    "\n",
    "Quiz question: For a tree trained on any dataset using max_depth = 6, min_node_size = 100, min_error_reduction=0.0, \n",
    "    what is the maximum number of splits encountered while making a single prediction?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nprint(\\'Quiz question: For my_decision_tree_new trained with max_depth = 6, min_node_size = 100, min_error_reduction=0.0, is the prediction path for validation_set[0] shorter, longer, or the same as for my_decision_tree_old that ignored the early stopping conditions 2 and 3?\\')\\nprint(\\'same\\')\\n\\n#20. Now, let\\'s use this function to evaluate the classification error of my_decision_tree_new on the \\n#validation_set. Your code should be analogous to\\nerr_new_tree = evaluate_classification_error(my_decision_tree_new, validation_set, target)\\nerr_old_tree = evaluate_classification_error(my_decision_tree_old, validation_set, target)\\nprint(\\'Validation error new tree:\\', err_new_tree)\\nprint(\\'*************************************************\\')\\nprint(\\'*************************************************\\')\\nprint(\\'Validation error old tree:\\', err_old_tree)\\n\\n\\n#Exploring the effect of max_depth\\n#We will compare three models trained with different values of the stopping criterion. We intentionally \\n#picked models at the extreme ends (too small, just right, and too large).\\n\\n#22. Train three models with these parameters:\\n\\n#model_1: max_depth = 2 (too small)\\n#model_2: max_depth = 6 (just right)\\n#model_3: max_depth = 14 (may be too large)\\n#For each of these three, set min_node_size = 0 and min_error_reduction = -1. Make sure to call \\n#the models model_1, model_2, and model_3.\\n\\n#Note: Each tree can take up to a few minutes to train. In particular, model_3 will probably take the longest to train.\\n\\n#23. Let us evaluate the models on the train and validation data. Let us start by evaluating the classification \\n#error on the training data. Your code should be analogous to:\\n\\n\\nmodel_1 = decision_tree_create(train_data, features, target, max_depth=2, min_node_size=0, min_error_reduction=-1)\\nmodel_2 = decision_tree_create(train_data, features, target, max_depth=6, min_node_size=0, min_error_reduction=-1)\\nmodel_3 = decision_tree_create(train_data, features, target, max_depth=14, min_node_size=0, min_error_reduction=-1)\\nmodel_1_TrainErr = evaluate_classification_error(model_1, train_data, target)\\nmodel_2_TrainErr =  evaluate_classification_error(model_2, train_data, target)\\nmodel_3_TrainErr =  evaluate_classification_error(model_3, train_data, target)\\nmodel_1_ValidationErr = evaluate_classification_error(model_1, validation_set, target)\\nmodel_2_ValidationErr =  evaluate_classification_error(model_2, validation_set, target)\\nmodel_3_ValidationErr =  evaluate_classification_error(model_3, validation_set, target)\\nprint(\"Training data, classification error (model 1):\", model1_TrainErr)\\nprint(\"Training data, classification error (model 2):\", model2_TrainErr)\\nprint(\"Training data, classification error (model 3):\", model3_TrainErr)\\nprint(\"Validation Set, classification error (model 1):\", model1_ValidationErr)\\nprint(\"Validation Set, classification error (model 2):\", model2_ValidationErr)\\nprint(\"Validation Set, classification error (model 3):\", model3_ValidationErr)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "print('Quiz question: For my_decision_tree_new trained with max_depth = 6, min_node_size = 100, \\\n",
    "min_error_reduction=0.0, is the prediction path for validation_set[0] shorter, longer, or the same \\\n",
    "as for my_decision_tree_old that ignored the early stopping conditions 2 and 3?')\n",
    "print('same')\n",
    "\n",
    "#20. Now, let's use this function to evaluate the classification error of my_decision_tree_new on the \n",
    "#validation_set. Your code should be analogous to\n",
    "err_new_tree = evaluate_classification_error(my_decision_tree_new, validation_set, target)\n",
    "err_old_tree = evaluate_classification_error(my_decision_tree_old, validation_set, target)\n",
    "print('Validation error new tree:', err_new_tree)\n",
    "print('*************************************************')\n",
    "print('*************************************************')\n",
    "print('Validation error old tree:', err_old_tree)\n",
    "\n",
    "\n",
    "#Exploring the effect of max_depth\n",
    "#We will compare three models trained with different values of the stopping criterion. We intentionally \n",
    "#picked models at the extreme ends (too small, just right, and too large).\n",
    "\n",
    "#22. Train three models with these parameters:\n",
    "\n",
    "#model_1: max_depth = 2 (too small)\n",
    "#model_2: max_depth = 6 (just right)\n",
    "#model_3: max_depth = 14 (may be too large)\n",
    "#For each of these three, set min_node_size = 0 and min_error_reduction = -1. Make sure to call \n",
    "#the models model_1, model_2, and model_3.\n",
    "\n",
    "#Note: Each tree can take up to a few minutes to train. In particular, model_3 will probably take the longest to train.\n",
    "\n",
    "#23. Let us evaluate the models on the train and validation data. Let us start by evaluating the classification \n",
    "#error on the training data. Your code should be analogous to:\n",
    "\n",
    "\n",
    "model_1 = decision_tree_create(train_data, features, target, max_depth=2, min_node_size=0, min_error_reduction=-1)\n",
    "model_2 = decision_tree_create(train_data, features, target, max_depth=6, min_node_size=0, min_error_reduction=-1)\n",
    "model_3 = decision_tree_create(train_data, features, target, max_depth=14, min_node_size=0, min_error_reduction=-1)\n",
    "model_1_TrainErr = evaluate_classification_error(model_1, train_data, target)\n",
    "model_2_TrainErr =  evaluate_classification_error(model_2, train_data, target)\n",
    "model_3_TrainErr =  evaluate_classification_error(model_3, train_data, target)\n",
    "model_1_ValidationErr = evaluate_classification_error(model_1, validation_set, target)\n",
    "model_2_ValidationErr =  evaluate_classification_error(model_2, validation_set, target)\n",
    "model_3_ValidationErr =  evaluate_classification_error(model_3, validation_set, target)\n",
    "print(\"Training data, classification error (model 1):\", model1_TrainErr)\n",
    "print(\"Training data, classification error (model 2):\", model2_TrainErr)\n",
    "print(\"Training data, classification error (model 3):\", model3_TrainErr)\n",
    "print(\"Validation Set, classification error (model 1):\", model1_ValidationErr)\n",
    "print(\"Validation Set, classification error (model 2):\", model2_ValidationErr)\n",
    "print(\"Validation Set, classification error (model 3):\", model3_ValidationErr)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
