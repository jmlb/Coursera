{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting a decision stump\n",
    "\n",
    "In this homework you will implement your own boosting module.\n",
    "\n",
    "Brace yourselves! This is going to be a fun and challenging assignment.\n",
    "\n",
    "Use SFrames to do some feature engineering.\n",
    "Train a boosted ensemble of decision-trees (gradient boosted trees) on the lending club dataset.\n",
    "Predict whether a loan will default along with prediction probabilities (on a validation set).\n",
    "Evaluate the trained model and compare it with a baseline.\n",
    "Find the most positive and negative loans using the learned model.\n",
    "Explore how the number of trees influences classification performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in original data file: 68\n",
      "Number of features in selected columns: 4\n"
     ]
    }
   ],
   "source": [
    "#We will be using a dataset from the LendingClub.\n",
    "#1. Load the dataset into a data frame named loans.\n",
    "#Extracting the target and the feature columns\n",
    "#2. We will now repeat some of the feature processing steps that we saw in the previous assignment:\n",
    "#First, we re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan.\n",
    "#Next, we select four categorical features:\n",
    "#grade of the loan\n",
    "#the length of the loan term\n",
    "#the home ownership status: own, mortgage, rent\n",
    "#number of years of employment.\n",
    "dataFile = r'lending-club-data.csv'\n",
    "#1. Load in the LendingClub dataset \n",
    "loans = pd.read_csv(dataFile, header=0, low_memory=False)\n",
    "#2. Reassign the labels to have +1 for a safe loan, and -1 for a risky (bad) loan.\n",
    "#The target column (label column) of the dataset that we are interested in is \n",
    "#called bad_loans. In this column 1means a risky (bad) loan 0 means a safe loan.\n",
    "#In order to make this more intuitive and consistent with the lectures, we reassign the target to be:\n",
    "#+1 as a safe loan\n",
    "#-1 as a risky (bad) loan\n",
    "#3. We put this in a new column called safe_loans.\n",
    "\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "#delete column 'bad_loans'\n",
    "loans = loans.drop('bad_loans', 1)\n",
    "\n",
    "#Exploring some features\n",
    "#2. Let's quickly explore what the dataset looks like. \n",
    "#First, print out the column names to see what features we have in this dataset.\n",
    "features = loans.columns.values\n",
    "print('Number of features in original data file:', np.shape(features)[0])\n",
    "\n",
    "#Selecting features\n",
    "#In this assignment, we will be using a subset of features (categorical and numeric). \n",
    "#The features we will be using are described in the code comments below. \n",
    "#If you are a finance geek, the LendingClub website has a lot more details about these features.\n",
    "target = 'safe_loans'\n",
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "\n",
    "#Recall from the lectures that one \n",
    "#common approach to coping with missing values is to \n",
    "#skip observations that contain missing values.\n",
    "\n",
    "loans = loans[[target] + features].dropna()\n",
    "print('Number of features in selected columns:', loans.shape[1] -1 )\n",
    "#Apply one-hot encoding to loans. Your tool may have a function for one-hot encoding. \n",
    "#Alternatively, see #7 for implementation hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features after hot encoding: 25\n",
      "features liste after hot encoding: ['safe_loans' 'grade_A' 'grade_B' 'grade_C' 'grade_D' 'grade_E' 'grade_F'\n",
      " 'grade_G' 'term_ 36 months' 'term_ 60 months' 'home_ownership_MORTGAGE'\n",
      " 'home_ownership_OTHER' 'home_ownership_OWN' 'home_ownership_RENT'\n",
      " 'emp_length_1 year' 'emp_length_10+ years' 'emp_length_2 years'\n",
      " 'emp_length_3 years' 'emp_length_4 years' 'emp_length_5 years'\n",
      " 'emp_length_6 years' 'emp_length_7 years' 'emp_length_8 years'\n",
      " 'emp_length_9 years' 'emp_length_< 1 year' 'emp_length_n/a']\n"
     ]
    }
   ],
   "source": [
    "#Apply one-hot encoding to loans. \n",
    "\n",
    "loans = pd.get_dummies(loans)\n",
    "\n",
    "#Load the JSON files into the lists train_idx and test_idx.\n",
    "#Perform train/validation split using train_idx and test_idx. In Pandas, for instance:\n",
    "\n",
    "print('Number of features after hot encoding:', loans.shape[1] -1 )\n",
    "print('features liste after hot encoding:', loans.columns.values )\n",
    "\n",
    "#Split data into training and validation\n",
    "#8. We split the data into training data and test data.\n",
    "train_idx = json.load(open(r'module-8-assignment-2-train-idx.json')) \n",
    "test_idx = json.load(open(r'module-8-assignment-2-test-idx.json'))\n",
    "train_data = loans.iloc[train_idx]\n",
    "test_data = loans.iloc[test_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted decision trees\n",
    "\n",
    "7. Let's modify our decision tree code from Module 5 to support weighting of individual data points.\n",
    "\n",
    "Weighted error definition\n",
    "\n",
    "8. Consider a model with N data points with:\n",
    "\n",
    "Predictions ŷ_1, ..., ŷ_n\n",
    "Target y_1, ..., y_n\n",
    "Data point weights α_1, ..., α_n\n",
    "Then the weighted error is defined by:\n",
    "\n",
    "\n",
    "where 1[ y_i ≠ ŷ_i ] is an indicator function that is set to 1 if y_i ≠ ŷ_i.\n",
    "\n",
    "Write a function to compute weight of mistakes\n",
    "\n",
    "9. Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "\n",
    "labels_in_node: y_1, ..., y_n\n",
    "data_weights: Data point weights α_1, ..., α_n\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "\n",
    "\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "\n",
    "\n",
    "The function intermediate_node_weighted_mistakes should first compute two weights:\n",
    "\n",
    "WM(−1): weight of mistakes when all predictions are ŷ_i = −1 i.e. WM(α,−1)\n",
    "WM(+1): weight of mistakes when all predictions are ŷ_i = +1 i.e. WM(α,+1)\n",
    "where −1 and +1 are vectors where all values are -1 and +1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_negative = sum(data_weights[labels_in_node == +1])/sum(data_weights)\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_positive = sum(data_weights[labels_in_node == -1])/sum(data_weights)\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if weighted_mistakes_all_negative > weighted_mistakes_all_positive:\n",
    "        return ( weighted_mistakes_all_negative, -1 )\n",
    "    else:\n",
    "        return ( weighted_mistakes_all_positive, +1 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
