{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 12 08:14:04 2016\n",
    "\n",
    "Code source: Jean-Marc Beaujour (jmlbeaujour@gmail.com)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import subprocess\n",
    "# from PIL import Image\n",
    "# im = Image.open(\"dt.png\")\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFile = r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig3a/lending-club-data.csv'\n",
    "#products = pd.read_table(dataFile, sep=' ', header=None, names=colNames) # user_id gender  age  occupation    zip\n",
    "loans = pd.read_csv(dataFile, header=0, low_memory=False) \n",
    "\n",
    "trainIdxFile = r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig3a/module-5-assignment-1-train-idx.json'\n",
    "validationIdxFile = r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig3a/module-5-assignment-1-validation-idx.json'\n",
    "train_idx = json.load(open(trainIdxFile)) #open the json file as a string and parse it with json.load ==> a list\n",
    "validation_idx = json.load(open(validationIdxFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#print loans.columns.values #print the names of the columns\n",
    "#Exploring some features\n",
    "#column = 'bad_loans' is the name of the target column, where 1=risky(bad), 0 =means safe\n",
    "#We will reassign the target such that +1 as a safe loan -1 bad loan\n",
    "#remove the 'bad_loans' column\n",
    "#Let's explore the distribution of the column safe_loans. This gives a sense of how many and risky loans are present in the dataset\n",
    "#Print out the percentage of safe loan and risky loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x: 1 if x==0 else -1)\n",
    "\n",
    "loans = loans.drop('bad_loans', 1) #df.drop('column_name', axis=1, inplace=True)\n",
    "\n",
    "nbr_safe_loans = sum(loans['safe_loans']==1)\n",
    "nbr_risky_loans =abs(sum(loans['safe_loans']==-1))\n",
    "print 'Percentage of safe loan is', 100.0*(nbr_safe_loans)/(nbr_safe_loans+nbr_risky_loans), '| risky loan', /\n",
    "100.0*(nbr_risky_loans)/(nbr_safe_loans+nbr_risky_loans) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extract the features of interest\n",
    "#Extract the feature columns and target columns only\n",
    "#Apply one-hot encoding to loans : Hot encoding\n",
    "#This is because some columns values are letters: C, C4 etc, and sci-kit-learn's decision tree implementattion requires only \n",
    "#numerical values for its data matrix. This means you will have to turn categorical variables into binary features via \n",
    "#one-hot encoding\n",
    "#Perform train/validation split using train_idx and validation_idx\n",
    "#Check that the split is 80-20\n",
    "print 'Split of training/validation examples (percent):', 100*train_exples/(train_exples+validation_exples) , '|' , 100*validation_exples/(train_exples+validation_exples)\n",
    "#check class balance: i.e 50% safe loans, 50% bad loans in trainign sets\n",
    "#some examples were left out of the initial data to achieve class balance. In this case, class balance was obtained by kicking out some examples to get to 50% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',                    # grade of the loan\n",
    "            'sub_grade',                 # sub-grade of the loan\n",
    "            'short_emp',                 # one year or less of employment\n",
    "            'emp_length_num',            # number of years of employment\n",
    "            'home_ownership',            # home_ownership status: own, mortgage or rent\n",
    "            'dti',                       # debt to income ratio\n",
    "            'purpose',                   # the purpose of the loan\n",
    "            'term',                      # the term of the loan\n",
    "            'last_delinq_none',         # has borrower had a delinquincy\n",
    "            'last_major_derog_none',    # has borrower had 90 day or worse rating\n",
    "            'revol_util',                # percent of available credit being used\n",
    "            'total_rec_late_fee',        # total late fees received to day\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "\n",
    "loans = loans[features + [target]]\n",
    "\n",
    "for colName in features:\n",
    "    if loans[colName].dtype == object :\n",
    "        # Create a set of dummy variables from the sex variable\n",
    "        dummies = pd.get_dummies(loans[colName])\n",
    "        # Join the dummy variables to the main dataframe\n",
    "        loans = loans.join(dummies)\n",
    "        loans = loans.drop(colName, 1)\n",
    "\n",
    "train_data = loans.iloc[train_idx]\n",
    "validation_data = loans.iloc[validation_idx]\n",
    "print train_data.shape\n",
    "\n",
    "\n",
    "train_exples = 1.0*train_data.shape[0]\n",
    "validation_exples = 1.0*validation_data.shape[0]\n",
    "Nbr_of_risky = -1.0*sum(train_data[train_data[target]==-1][target])\n",
    "Nbr_of_safe = sum(train_data[train_data[target]==1][target])\n",
    "print 'Percentage of Risky Loans in dataset:',Nbr_of_risky/(Nbr_of_risky + Nbr_of_safe)*100,/\n",
    "'|Safe Loans:', Nbr_of_safe/(Nbr_of_risky + Nbr_of_safe)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train_data_matrix = np.array(train_data)\n",
    "#As we explored above, our data is disproportionally full of safe loans. \n",
    "#Let's create two datasets: one with just the safe loans (safe_loans_raw) \n",
    "#and one with just the risky loans (risky_loans_raw).\n",
    "#One way to combat class imbalance is to undersample the larger class until the class distribution is approximately half and half. \n",
    "#Here, we will undersample the larger class (safe loans) in order to balance out our dataset. \n",
    "#This means we are throwing away many data points. We used seed=1 so everyone gets the same results.\n",
    "\n",
    "#Since there are fewer risky loans than safe loans, find the ratio of the sizes\n",
    "#and use that percentage to undersample the safe loans.\n",
    "#Append the risky_loans with the downsampled version of safe_loans\n",
    "#You can verify now that loans_data is comprised of approximately 50% safe loans and 50% risky loans.\n",
    "#Note: There are many approaches for dealing with imbalanced data, \n",
    "#including some where we modify the learning algorithm. These approaches are beyond the scope of this course, \n",
    "#but some of them are reviewed in this paper. For this assignment, we use the simplest possible approach, \n",
    "#where we subsample the overly represented class to get a more balanced dataset. \n",
    "#In general, and especially when the data is highly imbalanced, we recommend using more advanced methods.\n",
    "\n",
    "\n",
    "#use the built-in scikit-learn decision tree learner (sklearn) to create a prediction model on the training data\n",
    "#Convert the training dat and validation data into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_target = np.asarray(train_data[target])\n",
    "train_data_matrix = np.asarray(train_data.drop(target,1))\n",
    "validation_data_target = np.asarray(validation_data[target])\n",
    "validation_data_matrix = np.asarray(validation_data.drop(target,1))\n",
    "\n",
    "decision_tree_model = tree.DecisionTreeClassifier(max_depth=6)\n",
    "small_model = tree.DecisionTreeClassifier(max_depth=2)\n",
    "all_featuresName = (train_data.drop(target,1)).columns\n",
    "decision_tree_model = decision_tree_model.fit(train_data_matrix, train_data_target)\n",
    "small_model = small_model.fit(train_data_matrix, train_data_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vizualization of the learned model\n",
    "#GraphViz + sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dotfile = open(r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig3a/dtree2.dot', 'w')\n",
    "f = tree.export_graphviz(small_model, out_file = dotfile, feature_names = all_featuresName, \\\n",
    "                         class_names =['risky', 'safe'], filled=True, rounded=True, special_characters=True) \n",
    "dotfile.close()\n",
    "pngFile =r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig3a/dtree2.png'\n",
    "dFile = r'E:/DataScientist/myNotebook/ML_classification (Uni.Washington)/assig3a/dtree2.dot'\n",
    "dotExe = r'C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe'\n",
    "subprocess.call([dotExe, '-Tpng', dFile, '-o', pngFile])\n",
    "# ipython notebookImage(graph.create_png())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Making predictions\n",
    "#let's consider 2 positive and 2 negative examples from the validation set and see what the model predicts. We will do the following:\n",
    "#Predict whether or not a loan is safe\n",
    "#Predict the probability that a loan is safe\n",
    "\n",
    "#Let's take 2 positive examples and 2 negative examples\n",
    "#Now, we will use our model to predict whether or not a loan is likely to default.\n",
    "#For each row in the sample_validation_data, use the decision_tree_model to predict \n",
    "#whether or not the loan is classified as a safe loan. \n",
    "#(Hint: if you are using scikit-learn, you can use the .predict() method)\n",
    "\n",
    "#Explore probability predictions\n",
    "#For each row in the sample_validation_data, what is the probability (according decision_tree_model) of a loan being classified as safe? \n",
    "#(Hint: if you are using scikit-learn, you can use the .predict_proba() method)\n",
    "#thsi a 2D array [-1 1] probability\n",
    "#DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, ..., K-1]) classification.\n",
    "#+ 1 since np.array indexing starts at 0\n",
    "#Checkpoint: Can you verify that for all the predictions with probability >= 0.5, the model predicted the label +1?\n",
    "#Tricky predictions!\n",
    "#14. Now, we will explore something pretty interesting. For each row in the sample_validation_data, \n",
    "#what is the probability (according to small_model) of a loan being classified as safe?\n",
    "#Visualize the prediction on a tree\n",
    "#Note that you should be able to look at the small tree (of depth 2), \n",
    "#traverse it yourself, and visualize the prediction being made. \n",
    "#Consider the following point in the sample_validation_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_validation_safe_loans = validation_data[validation_data[target]==1].iloc[0:2]\n",
    "sample_validation_risky_loans = validation_data[validation_data[target]==-1].iloc[0:2]\n",
    "sample_validation_data = sample_validation_safe_loans.append(sample_validation_risky_loans)\n",
    "sample_validation_data_noLabel = sample_validation_data.drop(target,1)\n",
    "prediction_smallValidationData = decision_tree_model.predict(sample_validation_data_noLabel)\n",
    "result_smallValidation = (prediction_smallValidationData == np.array(sample_validation_data[target]))\n",
    "print 'What percentage of the predictions on sample_validation_data did decision_tree_model get correct?', \\\n",
    "100.0*sum((result_smallValidation==1).astype(int))/result_smallValidation.shape[0]\n",
    "\n",
    "proba_smallValidationData = decision_tree_model.predict_proba(sample_validation_data_noLabel) \n",
    "\n",
    "print 'what is the probability (according to decision_tree_model) of a loan being classified as safe?', proba_smallValidationData[:,1]\n",
    "print 'Quiz Question: Which loan has the highest probability of being classified as a safe loan?', np.argmax(proba_smallValidationData[:,1])+1\n",
    "\n",
    "expected_target_smallValidation = np.vectorize(lambda x: 1 if x>=0.5 else -1)(proba_smallValidationData[:,1])\n",
    "comparison = (prediction_smallValidationData == expected_target_smallValidation)\n",
    "print 'Can you verify that for all the predictions with probability >= 0.5, the model predicted the label +1', comparison\n",
    "\n",
    "proba_smallModel_smallValidationData = small_model.predict_proba(sample_validation_data_noLabel)\n",
    "\n",
    "print 'what is the probability (according to small_model) of a loan being classified as safe?', proba_smallModel_smallValidationData[:,1]\n",
    "print 'Quiz Question: Notice that the probability preditions are the exact same for the 2nd and 3rd loans. Why would this happen? the features up to depth 2 are the same'\n",
    "same_percent_samples = sample_validation_data_noLabel\n",
    "print same_percent_samples\n",
    "\n",
    "validation_point = sample_validation_data.iloc[1]\n",
    "print validation_point\n",
    "print 'Quiz Question:\\n' \n",
    "print 'Based on the visualized tree, what prediction would you make for this data point (according to small_model)? Risky loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, verify your prediction by examining the prediction made using small_model.\n",
    "#Evaluating accuracy of the decision tree model\n",
    "#Recall that the accuracy is defined as follows: accuracy = correctly classified examples/total examples\n",
    "#Evaluate the accuracy of small_model and decision_tree_model on the training data. \n",
    "#(Hint: if you are using scikit-learn, you can use the .score() method)\n",
    "#score Returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "#Checkpoint: You should see that the small_model performs worse than the decision_tree_model on the training data.\n",
    "\n",
    "#Evaluating accuracy of a complex decision tree model\n",
    "#Here, we will train a large decision tree with max_depth=10. \n",
    "#This will allow the learned tree to become very deep, and result in a very complex model. \n",
    "#Recall that in lecture, we prefer simpler models with similar predictive power. \n",
    "#This will be an example of a more complicated model which has similar predictive power, i.e. something we don't want.\n",
    "\n",
    "#Evaluate the accuracy of big_model on the training set and validation set.\n",
    "#Checkpoint: We should see that big_model has even better performance on the training set than decision_tree_model did on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_train_decisionTreeModel = decision_tree_model.score(train_data_matrix, train_data_target)\n",
    "score_train_smallModel = small_model.score(train_data_matrix, train_data_target)\n",
    "print 'Evaluate the accuracy on the training data of decision_tree_model', score_train_decisionTreeModel,'| and small model ',  score_train_smallModel\n",
    "\n",
    "score_validation_decisionTreeModel = decision_tree_model.score(validation_data_matrix, validation_data_target)\n",
    "score_validation_smallModel = small_model.score(validation_data_matrix, validation_data_target)\n",
    "print '\\n Evaluate the accuracy on the validation data of decision_tree_model', score_validation_decisionTreeModel,'| and small model ',  score_validation_smallModel\n",
    "\n",
    "print '**** Quiz Question:****'\n",
    "print '\\nWhat is the accuracy of decision_tree_model on the validation set (rounded to the nearest .01): %.2f?' % score_validation_decisionTreeModel\n",
    "\n",
    "\n",
    "big_model = tree.DecisionTreeClassifier(max_depth=10)\n",
    "big_model = big_model.fit(train_data_matrix, train_data_target)\n",
    "big_model_train_score = big_model.score(train_data_matrix, train_data_target)\n",
    "big_model_validation_score = big_model.score(validation_data_matrix, validation_data_target)\n",
    "\n",
    "print '\\n Evaluate the accuracy of big_model on: the training set %.2f|validation set: %.2f' % (big_model_train_score, big_model_validation_score)\n",
    "\n",
    "print '**** Quiz Question:****'\n",
    "print '\\n How does the performance of big_model on the validation set compare to decision_tree_model on the validation set?'\n",
    "print '\\n answer: slightly worst. This is a sign of overfitting, as accuracy increases on training set with big model'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Quantifying the cost of mistakes\n",
    "#Every mistake the model makes costs money. In this section, \n",
    "#we will try and quantify the cost each mistake made by the model. Assume the following:\n",
    "#False negatives: Loans that were actually safe but were predicted to be risky. \n",
    "#This results in an oppurtunity cost of loosing a loan that would have otherwise been accepted.\n",
    "#False positives: Loans that were actually risky but were predicted to be safe. \n",
    "#These are much more expensive because it results in a risky loan being given.\n",
    "#Correct predictions: All correct predictions don't typically incur any cost.\n",
    "#Let's write code that can compute the cost of mistakes made by the model. Complete the following 4 steps:\n",
    "\n",
    "#First, let us compute the predictions made by the model\n",
    "\n",
    "#Third, compute the number of false negatives..\n",
    "#Finally, compute the cost of mistakes made by the model by adding up the costs of true positives and false positves.\n",
    "#Quiz Question: Let's assume that each mistake costs us money: a false negative costs $10,000, while a false positive \n",
    "positive costs $20,000. What is the total cost of mistakes made by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_data_predict = decision_tree_model.predict(validation_data_matrix)\n",
    "print 'size', validation_data_matrix.shape[0]\n",
    "#Second, compute the number of false positives: i.e loans predicted 1 (safe) but target (-1) risky\n",
    "validation_predict_target = pd.concat([pd.DataFrame({'predict': validation_data_predict}), pd.DataFrame({'target': validation_data_target})], axis=1)\n",
    "false_positive = validation_predict_target[validation_predict_target['predict']==1]\n",
    "count_false_positive = false_positive[false_positive['target']==-1].shape[0]\n",
    "print 'The number of False positive is:', count_false_positive  \n",
    "\n",
    "false_negative = validation_predict_target[validation_predict_target['predict']==-1]\n",
    "count_false_negative = false_negative[false_negative['target']==1].shape[0]\n",
    "print 'The number of False negative is:', count_false_negative\n",
    "decision_tree_model on validation_data?\n",
    "cost_error = 10000*count_false_negative + 20000*count_false_positive\n",
    "print '*********** Quiz Question ************'\n",
    "print '\\nThe total cost of mistakes made by decision_tree_model on validation_data?: %.2f' % cost_error\n",
    "print '\\nThe cost of mistakes per application made by decision_tree_model on validation_data?: %.2f' % (cost_error/validation_data_matrix.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
