\relax 
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Week 1 and 2}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Introduction / Definition }{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}What is Machine Learning?}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Supervised Learning}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   The right answer in this example is the price corresponding to the house size. \relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{housingData}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   In classification problems, one can have more than 2 values for the output: for example may be 3 types of cancer (0=benign, 1=malignant, 2=type2, 3=type3). The graph on the right shows an alternative way of plotting the data when using several features / attributes. The learning algorithm may decide to separate the malign and benign with a straight line, hence the example of with shown age and tumor size would have a high probability to be a malign tumor \relax }}{3}}
\newlabel{breastcancer}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Supervised vs. Unsupervised Learning}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Illustration of supervised and unsupervised learning\relax }}{3}}
\newlabel{superVSunsuper}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces This is an example of density estimation. The $x$ axis has a distribution of data point centered at -2, 1 and 6\relax }}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Generative vs. Discriminative models}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Discriminative models}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.2}Generative model}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Supervised Learning: Linear regression with 1 variable/feature (aka univariable linear regression)}{4}}
\newlabel{hypo}{{2}{4}}
\newlabel{errfn}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Application to the house pricing example}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Result: the red line is the best fit from Gradient descent.  \relax }}{6}}
\newlabel{fig2}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Multiple features}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}Hypothesis function for $n$ features}{6}}
\newlabel{h_multi}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2} The Cost function $J(\theta )$}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.3}Gradient descent algorithm}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Gradient descent in practice}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.1}Feature scaling: how to converge faster}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.2}Learning rate $\alpha $: how to converge faster}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   If gradient descent works, $J(\theta )$ must decrease after each iteration (3). If $\alpha $ is too small, Gradient Descent will slowly converge (1). But if $\alpha $ too large, Gradient descent woudl not converge ((case 2) or $J(\theta )$ increases) \relax }}{9}}
\newlabel{learningRate}{{6}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.3}Polynomial regression}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {0.6}Normal Equations}{9}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Linear Algebra Review}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}1-indexed versus 0-indexed vector matrix}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Matrix}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Matrix size}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}Matrix Operation}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix addition}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix scalar multiplication}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Multiplication of 2 matrix}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix Identity}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix Inverse}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Transpose matrix $A^{\mathrm  {T}}$}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.3}Derivative $\nabla _A f(A)$}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.4}Trace}{15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Matlab}{16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Exercise: Gradient Descent Matlab}{18}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
