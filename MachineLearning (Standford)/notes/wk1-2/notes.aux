\relax 
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Week 1 and 2}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Introduction / Definition }{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}What is Machine Learning?}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Supervised Learning}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   The right answer in this example is the price corresponding to the house size. \relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{housingData}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   In classification problems, one can have more than 2 values for the output: for example may be 3 types of cancer (0=benign, 1=malignant, 2=type2, 3=type3). The graph on the right shows an alternative way of plotting the data when using several features / attributes. The learning algorithm may decide to separate the malign and benign with a straight line, hence the example of with shown age and tumor size would have a high probability to be a malign tumor \relax }}{3}}
\newlabel{breastcancer}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Supervised vs. Unsupervised Learning}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Illustration of supervised and unsupervised learning\relax }}{3}}
\newlabel{superVSunsuper}{{3}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Supervised Learning: Linear regression with 1 variable/feature (aka univariable linear regression)}{3}}
\newlabel{hypo}{{1}{3}}
\newlabel{errfn}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Application to the house pricing example}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Result: the red line is the best fit from Gradient descent.  \relax }}{5}}
\newlabel{fig2}{{4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Multiple features}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Hypothesis function for $n$ features}{5}}
\newlabel{h_multi}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2} The Cost function $J(\theta )$}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.3}Gradient descent algorithm}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Gradient descent in practice}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}Feature scaling: how to converge faster}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2}Learning rate $\alpha $: how to converge faster}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces   If gradient descent works, $J(\theta )$ must decrease after each iteration (3). If $\alpha $ is too small, Gradient Descent will slowly converge (1). But if $\alpha $ too large, Gradient descent woudl not converge ((case 2) or $J(\theta )$ increases) \relax }}{8}}
\newlabel{learningRate}{{5}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.3}Polynomial regression}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Normal Equations}{8}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Linear Algebra Review}{12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}1-indexed versus 0-indexed vector matrix}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Matrix}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Matrix size}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}Matrix Operation}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix addition}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix scalar multiplication}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Multiplication of 2 matrix}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix Identity}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix Inverse}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Transpose matrix $A^{\mathrm  {T}}$}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.3}Derivative $\nabla _A f(A)$}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.4}Trace}{14}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Matlab}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Exercise: Gradient Descent Matlab}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
